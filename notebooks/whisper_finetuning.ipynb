{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Whisper Fine-Tuning Workflow\n",
        "\n",
        "Use this notebook after exporting reviewed transcriptions with `python -m wavecap_backend.tools.export_transcriptions` to build a dataset that can fine-tune Whisper models with Hugging Face Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "1. Install the Python dependencies used below: `pip install datasets transformers accelerate soundfile`.\n",
        "2. Run the export script from the repository root (or from `backend/` with `PYTHONPATH=src`) to generate a dataset directory:\n",
        "   ```bash\n",
        "   python -m wavecap_backend.tools.export_transcriptions --output-dir state/exports/whisper-dataset\n",
        "   ```\n",
        "3. Confirm that the output directory contains `transcriptions.jsonl`, `metadata.json`, and an `audio/` sub-directory when audio was copied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the reviewed transcriptions\n",
        "\n",
        "The export script produces a JSONL file where each record contains the cleaned text, metadata about the review, and an `audio_filepath` pointing to the WAV clip.\n",
        "The code below loads the JSONL file into a \ud83e\udd17 `datasets` `DatasetDict`, filters out entries without audio, and casts the audio column so it can stream waveform arrays on demand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from datasets import Audio, load_dataset\n",
        "\n",
        "DATASET_DIR = Path(\"state/exports/whisper-dataset\")\n",
        "JSONL_PATH = DATASET_DIR / \"transcriptions.jsonl\"\n",
        "\n",
        "if not JSONL_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Could not find {JSONL_PATH}. Run the export script first.\")\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=str(JSONL_PATH), split=\"train\")\n",
        "dataset = dataset.filter(lambda example: example.get(\"audio_filepath\") is not None)\n",
        "dataset = dataset.cast_column(\"audio_filepath\", Audio(sampling_rate=16000))\n",
        "dataset = dataset.rename_column(\"audio_filepath\", \"audio\")\n",
        "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect a single example to verify the metadata and waveform are loaded as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = dataset[\"train\"][0]\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare inputs for Whisper\n",
        "\n",
        "Load the Whisper processor and model checkpoint, then convert each dataset example into the input features and token labels expected by the model. Adjust the `language` and `task` arguments to match your recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "\n",
        "MODEL_NAME = \"openai/whisper-small\"\n",
        "TARGET_LANGUAGE = \"en\"\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=TARGET_LANGUAGE, task=\"transcribe\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
        "processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_example(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    input_features = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    with processor.as_target_processor():\n",
        "        labels = processor.tokenizer(batch[\"text\"])\n",
        "    batch[\"input_features\"] = input_features\n",
        "    batch[\"labels\"] = labels[\"input_ids\"]\n",
        "    return batch\n",
        "\n",
        "processed_dataset = dataset.map(\n",
        "    prepare_example,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    num_proc=None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tune Whisper\n",
        "\n",
        "Configure standard sequence-to-sequence training arguments and launch the trainer. Depending on dataset size you may need to adjust batch sizes, gradient accumulation, or enable mixed precision (`fp16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"whisper-finetune\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    fp16=False,\n",
        "    gradient_checkpointing=True,\n",
        "    generation_max_length=225,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=processor.tokenizer, model=model, padding=True)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset[\"train\"],\n",
        "    eval_dataset=processed_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor,\n",
        ")\n",
        "\n",
        "# Uncomment the next line to begin fine-tuning once you are satisfied with the configuration.\n",
        "# trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training completes you can evaluate the model, push it to the Hugging Face Hub, or convert it back into a format suitable for the WaveCap backend."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
